"""See OpenAI's docs: https://platform.openai.com/docs/api-reference/realtime

https://platform.openai.com/docs/api-reference/realtime-client-events
https://platform.openai.com/docs/api-reference/realtime-server-events
"""

import datetime as dt
import uuid
from typing import (
    Any,
    Generic,
    Literal,
    TypeVar,
    Union,
    get_args,
    get_origin,
)
from uuid import UUID

from pydantic import BaseModel, Field, model_validator

T = TypeVar("T", bound=str)

ResponsesLenght = Literal["XS", "S", "M", "L", "XL"]


class BaseEvent(BaseModel, Generic[T]):
    type: T = None  # type: ignore - will be set by validator below
    event_id: str = Field(default_factory=lambda: "event_" + str(uuid.uuid4().hex))

    @model_validator(mode="after")
    def set_type_from_generic(self) -> "BaseEvent":
        if type(self) == BaseEvent:  # noqa - we want to use type() == and not isinstance
            raise ValueError("Cannot instantiate BaseEvent directly")

        # e.g. Literal["session.update"]
        type_argument = self.__class__.model_fields["type"].annotation

        if get_origin(type_argument) is not Literal:
            # I don't know how to enforce this in the type system directly
            raise ValueError("Type argument is not a Literal")

        self.type = get_args(type_argument)[0]  # e.g. "session.update"

        return self


class ErrorDetails(BaseModel):
    type: str
    code: str | None = None
    message: str
    param: str | None = None
    # ours, not part of the OpenAI API:
    details: object | None = None


class Error(BaseEvent[Literal["error"]]):
    error: ErrorDetails


class InputAudioBufferAppend(BaseEvent[Literal["input_audio_buffer.append"]]):
    audio: str  # Base64-encoded Opus data


class InputAudioBufferSpeechStarted(
    BaseEvent[Literal["input_audio_buffer.speech_started"]]
):
    """Speech started according to the STT.

    Note this is not symmetrical with `InputAudioBufferSpeechStopped` because it's based
    on the STT and not the VAD signal. This is because sometimes the VAD will think
    there is speech but then nothing will end up getting transcribed. If we were using
    the VAD for both events we might get a start event without a stop event.
    For VAD interruptions, see `UnmuteInterruptedByVAD`.
    """


class InputAudioBufferSpeechStopped(
    BaseEvent[Literal["input_audio_buffer.speech_stopped"]]
):
    """A pause was detected by the VAD."""


class Response(BaseModel):
    object: Literal["realtime.response"] = "realtime.response"
    # We currently only use in_progress
    status: Literal["in_progress", "completed", "cancelled", "failed", "incomplete"]
    voice: str


class ResponseCreated(BaseEvent[Literal["response.created"]]):
    response: Response


class ResponseTextDelta(BaseEvent[Literal["response.text.delta"]]):
    delta: str
    response_id: UUID


class OneResponse(BaseEvent[Literal["one.response"]]):
    content: str
    timestamp: dt.datetime  # Datetime when response generation started
    index: int  # 0-3 to design which response it is, 0 being the first one


class OneKeyword(BaseEvent[Literal["one.keyword"]]):
    content: str
    timestamp: dt.datetime  # Datetime when response generation started
    index: int  # 0-9 to design which keyword it is, 0 being the first one


class ResponseAudioDelta(BaseEvent[Literal["response.audio.delta"]]):
    delta: str  # Base64-encoded Opus audio data
    response_id: UUID


class ResponseSelectedByWriter(BaseEvent[Literal["response.selected.by.writer"]]):
    text: str
    id: UUID


class CurrentKeywords(BaseEvent[Literal["current.keywords"]]):
    keywords: str | None


class DesiredResponsesLenght(BaseEvent[Literal["desired.responses.length"]]):
    length: ResponsesLenght


class ResponseAudioDone(BaseEvent[Literal["response.audio.done"]]):
    pass


class TranscriptLogprob(BaseModel):
    bytes: bytes
    logprob: float
    token: str


class ConversationItemInputAudioTranscriptionDelta(
    BaseEvent[Literal["conversation.item.input_audio_transcription.delta"]]
):
    delta: str
    start_time: float  # Unmute extension


class UnmuteAdditionalOutputs(BaseEvent[Literal["unmute.additional_outputs"]]):
    args: Any


class UnmuteResponseTextDeltaReady(
    BaseEvent[Literal["unmute.response.text.delta.ready"]]
):
    delta: str
    response_id: UUID


class UnmuteResponseAudioDeltaReady(
    BaseEvent[Literal["unmute.response.audio.delta.ready"]]
):
    number_of_samples: int


class UnmuteInterruptedByVAD(BaseEvent[Literal["unmute.interrupted_by_vad"]]):
    """The VAD interrupted the response generation."""


# Server events (from OpenAI to client)
ServerEvent = Union[
    Error,
    ResponseTextDelta,
    ResponseAudioDelta,
    ResponseAudioDone,
    ResponseCreated,
    OneResponse,
    OneKeyword,
    ConversationItemInputAudioTranscriptionDelta,
    InputAudioBufferSpeechStarted,
    InputAudioBufferSpeechStopped,
    UnmuteAdditionalOutputs,
    UnmuteResponseTextDeltaReady,
    UnmuteResponseAudioDeltaReady,
    UnmuteInterruptedByVAD,
]

# Client events (from client to OpenAI)
ClientEvent = Union[
    InputAudioBufferAppend,
    ResponseSelectedByWriter,
    CurrentKeywords,
    DesiredResponsesLenght,
]

Event = ClientEvent | ServerEvent
